{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "175f7e41-aea5-4c2c-a59c-b03b5afc3029",
   "metadata": {},
   "source": [
    "Everything in this notebooks is based on: https://arxiv.org/pdf/2005.14458\n",
    "\n",
    "\n",
    "# Distributional Regression Forest\n",
    "Distributional Regression Forest is a method based on the Random Forest algorithm (Breiman, 2001) which in a data-adaptive way determines for any given test point which training data points are relevant for it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65117f1-d47c-40c0-8004-e9fc65e152d9",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Let $Y = (Y_1, Y_2, . . . , Y_d) \\in \\mathbb{R}^d$ be a multivariate random variable representing the data of interest, but whose joint distribution is heterogeneous and depends on some subset of a potentially large number of covariates $X = (X_1, X_2, . . . , X_p)  \\in \\mathbb{R}^p$. We aim to estimate a certain target object $\\tau(x)$ that depends on the conditional distribution $P(Y | X=x) = P(Y | X_1 =x_1, . . . , X_p =x_p)$, where $x = (x_1, . . . , x_p)$ is an arbitrary point in $\\mathbb{R}^p$. The estimation target $\\tau(x)$ can range\n",
    "from simple quantities, such as the conditional expectations or quantiles for some function to some more complicated aspects of the conditional distribution, such as conditional copulas or conditional independence measures. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e332e-6618-491b-a1f4-45852ad37130",
   "metadata": {},
   "source": [
    "Given the observed data {$(x_i, y_i)$}$^{n}_{i=1}$, the most straightforward way of estimating $\\tau(x)$ nonparametrically would be to consider only the data points in some neighborhood $N_x$ around $x$, e.g. by considering the k-nearest neighbors according to some metric. However, such methods typically suffer from the curse of dimensionality even when $p$ is only moderately large: for a reasonably small neighborhood, such that the distribution $P(Y | X \\in N_x)$ is close to the distribution $P(Y | X=x)$, the number of training data points contained in $N_x$ will be very small, thus making the accurate estimation of the target $\\tau(x)$ difficult. For that reason, more importance should be given to the training data points $(x_i, y_i)$ for which the response distribution $P(Y | X=x_i)$ at point $x_i$ is similar to the target distribution $P(Y | X=x)$, even if $x_i$ is not necessarily close to $x$ in every component.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5866ccb-3ac2-42b0-b144-006674e16d3d",
   "metadata": {},
   "source": [
    "Distributional Random Forest (DRF) is an algorithm which estimates the multivariate conditional distribution in a locally adaptive fashion by repeatedly dividing the data points in the spirit of the Random Forest algorithm: at each step, we split the data points into two groups based on some feature $X_j$ in such a way that the distribution of $Y$ for which $X_j \\leq l$, for some level $l$, differs the most compared to the distribution of $Y$ when $X_j \\geq l$, according to some distributional metric. As the default choice, Maximal Mean Discrepancy (MMD) statistic is used as the metric. This splitting procedure partitions the data such that the\n",
    "distribution of the multivariate response Y in the resulting leaf nodes is as homogeneous as possible, thus defining neighborhoods of relevant training data points for every $x$. Repeating this many times with randomization induces a weighting function $w_x(x_i)$ which quantifies the relevance of each training data point $x_i$ for a given test point $x$. The conditional distribution is then estimated by an\n",
    "empirical distribution determined by these weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56508bc7-feba-4533-8f34-0893d28b0986",
   "metadata": {},
   "source": [
    "The induced weighting function can be used not only for obtaining simple distributional aspects such as, for example, the conditional quantiles, conditional correlations, or joint conditional probability statements, but also to obtain more complex objectives, such as conditional independence tests, heterogeneous regression or semiparametric estimation by fitting a parametric model for $Y$, having nonparametrically adjusted for $X$. Representation of the conditional distribution via the weighting function has a great potential for applications in causality such as causal effect estimation or as a way of implementing do-calculus for finite samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a8bdd1-c199-41ea-b672-9db9fce6ea0c",
   "metadata": {},
   "source": [
    "DRF is used in two steps: in the first step, we obtain the weighting function $w_x(·)$ describing the conditional distribution $P(Y | X=x)$ in a target- and model-free way, which is then used as an input for the second step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9958369-4ae4-41cf-b396-97d31ffba1f3",
   "metadata": {},
   "source": [
    "![Description of Image](Images%20and%20data/imagen_tesis1.png \"Optional Title\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ac1b3d-314d-4605-a7a0-bc63f4c5dad5",
   "metadata": {},
   "source": [
    "#### Forest Building\n",
    "The trees are grown recursively in a model-free and target-free way as follows: For every parent node $P$, we determine how to best split it into two child nodes of the form $C_L = {X_j \\leq l}$ and $C_R = {X_j > l}$, where the variable $X_j$ is one of the randomly chosen\n",
    "splitting candidates and $l$ denotes its level based on which we perform the splitting. The split is chosen such that we maximize a certain (multivariate) two-sample test statistic \n",
    "$$D({y_i| x_i \\in C_L} , {y_i| x_i ∈ C_R})$$ \n",
    "which measures the difference of the empirical distributions of the data $Y$ in the two resulting child nodes $C_L$ and $C_R$. Therefore, in each step we select the candidate predictor $X_j$ which seems to affect the distribution of $Y$ the most, as measured by the metric $D(·, ·)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72f2299-c416-40f9-89c3-d44e33ddcda2",
   "metadata": {},
   "source": [
    "#### Weighting Function\n",
    "Having constructed our forest, we can use the induced weighting function to estimate the conditional distribution at any given test point\n",
    "$x$ and thus any other quantity of interest $\\tau(x)$. Suppose that we have built $N$ trees $T_1, . . . , T_N$ . Let $L_k(x)$ be the set of the training data points which end up in the same leaf as $x$ in the tree $T_k$. The weighting function $w_x(x_i)$ is defined as the average of the corresponding weighting functions per tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dada9ed-5a3c-4f39-a95d-38413d5018c4",
   "metadata": {},
   "source": [
    "$$w_x(x_i) = \\frac{1}{N}\\sum_{k=1}^{N} \\frac{ \\mathbb{1} x_i \\in L_k(x)}{| L_k(x)|}$$\n",
    "\n",
    "The weights are positive and add up to 1:$\\sum_{i=1}^{n} w_x(x_i) = 1$. The sets $L_k(x)$ of DRF will contain data points $(x_i, y_i)$ such that $P(Y | X = x_i)$ is close to $P(Y | X = x)$, thus removing bias due to heterogeneity of $Y$ caused by $X$. On the other hand, since the trees are constructed randomly and are thus fairly independent, the leaf sets $L_k(x)$ will be different enough so that the induced weights $w_x(x_i)$ are not concentrated on a small set of data points, which would lead to high estimation variance.\n",
    "\n",
    "One can estimate the conditional distribution $P(Y | X = x)$ from the weighting function by using the corresponding empirical distribution:\n",
    "$$\\hat(P)(Y | X = x) = \\sum_{i=1}^{n} w_x(x_i) \\delta_{y_i}$$ \n",
    "where $ \\delta_{y_i}$ is the point mass at $y_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e714a6-2e61-4c2c-827c-43944e35362c",
   "metadata": {},
   "source": [
    "#### Distributional Metric\n",
    "In order to determine the best split of a parent node $P$, i.e. such that the distributions of the responses $Y$ in the resulting child nodes $C_L$ and $C_R$ differ the most, one needs a good distributional metric $D(·, ·)$ which can detect change in distribution of the response $Y$ when additionally conditioning on an event ${X_j > l}$. Additional requirement for the choice of distributional metric $D(·, ·)$ used for data splitting is that it needs to be computationally very efficient as splitting is used extensively in the algorithm.\n",
    "\n",
    "Even though DRF could in theory be constructed with any distributional metric $D(·, ·)$,as a default choice the splitting criterion based on the Maximum Mean Discrepancy (MMD) statistic is chosen. Let $(H,<·, ·>_{H})$ be the RKHS of real-valued functions on $\\mathbb{R}^d$ induced by some positive-definite kernel $k$, and let $\\Phi: \\mathbb{R}^d \\rightarrow H$ be the corresponding feature map satisfying that $k(u, v) =  <\\varphi(u),  \\varphi(v)>_H$. The MMD statistic $DMMD(k)(U, V )$ for kernel $k$ and two samples $U = {u_1, . . . , u_|U|}$ and\n",
    "$V = {v_1, . . . , v_|V |}$ is given by:\n",
    "\n",
    "$$DMMD_{(k)}(U, V ) = \\frac{1}{|U|^2} \\sum_{i,j=1}^{|U|}k(u_i, u_j ) + \\frac{1}{|V|^2} \\sum_{i,j=1}^{|V|}k(v_i, v_j) -\\frac{2}{|U||V|} \\\n",
    "\\sum_{i=1}^{|U|}\\sum_{j=1}^{|V|}k(u_i, v_j )$$\n",
    "\n",
    "MMD compares the similarities, described by the kernel $k$, within each sample with the similarities across samples and is commonly used in practice for two-sample testing. It is based on the idea that one can assign to each distribution $P$ its embedding $\\mu(P)$ into the\n",
    "RKHS $H$, which is the unique element of $H$ given by:\n",
    "\n",
    "$$\\mu(P)=\\mathbb{E}_{Y \\sim P} [\\varphi (Y)]$$ \n",
    "\n",
    "The MMD two-sample statistic can then equivalently be written as the squared distance between the embeddings of the empirical distributions with respect to the RKHS norm $||.||_{H}$:\n",
    "\n",
    "$$ DMMD_{(k)}(U, V ) =||\\mu(\\frac{1}{|U|} \\sum_{i=1}^{|U|}\\delta_{u_i})-\\mu(\\frac{1}{|V|} \\sum_{i=1}^{|V|}\\delta_{v_i})||^2_{H}$$\n",
    "\n",
    "recalling that $\\delta_y$ is the point mass at $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a650c4e-7c9b-4372-9598-a10c8b752fd8",
   "metadata": {},
   "source": [
    "The $O((|U| + |V |)^2)$ complexity for computing $DMMD_{(k)}(U, V )$ is nevertheless too large for many applications. For that\n",
    "reason, several fast approximations of MMD have been suggested in the literature and one of those is used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50113966-7932-47fc-b188-9a4311965905",
   "metadata": {},
   "source": [
    "## Application to real data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b3aec-6693-4f68-876f-2f8e6075f0fa",
   "metadata": {},
   "source": [
    "The data we have comes from Guanajuato, Mexico, and includes measurements of temperature, pressure, radiation, and humidity collected from 2014 to 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a722abf9-fdce-4d3c-99fe-f8a8d228171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from skimpy import skim\n",
    "from drf import drf\n",
    "#!pip install -i https://test.pypi.org/simple/ drf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0049ec80-8f0d-4fc4-83d6-eb3735ee5457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "data=pd.read_csv(\"Images and data/Distributional_regression_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e0e2c33-a6cc-4b71-9327-8bee38ddd323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n",
       "│ <span style=\"font-style: italic\">         Data Summary         </span> <span style=\"font-style: italic\">      Data Types       </span>                                                          │\n",
       "│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n",
       "│ ┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Dataframe         </span>┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Values </span>┃ ┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Column Type </span>┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Count </span>┃                                                          │\n",
       "│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n",
       "│ │ Number of rows    │ 310752 │ │ float64     │ 3     │                                                          │\n",
       "│ │ Number of columns │ 4      │ │ string      │ 1     │                                                          │\n",
       "│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n",
       "│ <span style=\"font-style: italic\">                                                    number                                                    </span>  │\n",
       "│ ┏━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━━━┓  │\n",
       "│ ┃<span style=\"font-weight: bold\"> column       </span>┃<span style=\"font-weight: bold\"> NA   </span>┃<span style=\"font-weight: bold\"> NA %                 </span>┃<span style=\"font-weight: bold\"> mean   </span>┃<span style=\"font-weight: bold\"> sd     </span>┃<span style=\"font-weight: bold\"> p0    </span>┃<span style=\"font-weight: bold\"> p25  </span>┃<span style=\"font-weight: bold\"> p50  </span>┃<span style=\"font-weight: bold\"> p75   </span>┃<span style=\"font-weight: bold\"> p100 </span>┃<span style=\"font-weight: bold\"> hist   </span>┃  │\n",
       "│ ┡━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━━━┩  │\n",
       "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">Humedad_rel </span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 291</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 0.09364380599320359</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 62.07</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 26.42</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  3.9</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  40</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">65.1</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 85.9</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 100</span> │ <span style=\"color: #008000; text-decoration-color: #008000\">▂▄▄▅▅█</span> │  │\n",
       "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">Temp_aire   </span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 348</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 0.11198640716713006</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 16.59</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 6.374</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">-17.5</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">12.4</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">16.2</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 21.4</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">35.2</span> │ <span style=\"color: #008000; text-decoration-color: #008000\">  ▂█▆▁</span> │  │\n",
       "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">Rad_Solar   </span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 213</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 0.06854340438677788</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 154.5</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 217.2</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">    0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 1.5</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">292.1</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">1094</span> │ <span style=\"color: #008000; text-decoration-color: #008000\"> █▁▂▁ </span> │  │\n",
       "│ └──────────────┴──────┴──────────────────────┴────────┴────────┴───────┴──────┴──────┴───────┴──────┴────────┘  │\n",
       "│ <span style=\"font-style: italic\">                                                    string                                                    </span>  │\n",
       "│ ┏━━━━━━━━┳━━━━┳━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┓  │\n",
       "│ ┃<span style=\"font-weight: bold\">        </span>┃<span style=\"font-weight: bold\">    </span>┃<span style=\"font-weight: bold\">      </span>┃<span style=\"font-weight: bold\">            </span>┃<span style=\"font-weight: bold\">           </span>┃<span style=\"font-weight: bold\">            </span>┃<span style=\"font-weight: bold\">           </span>┃<span style=\"font-weight: bold\"> chars per  </span>┃<span style=\"font-weight: bold\"> words per </span>┃<span style=\"font-weight: bold\"> total      </span>┃  │\n",
       "│ ┃<span style=\"font-weight: bold\"> column </span>┃<span style=\"font-weight: bold\"> NA </span>┃<span style=\"font-weight: bold\"> NA % </span>┃<span style=\"font-weight: bold\"> shortest   </span>┃<span style=\"font-weight: bold\"> longest   </span>┃<span style=\"font-weight: bold\"> min        </span>┃<span style=\"font-weight: bold\"> max       </span>┃<span style=\"font-weight: bold\"> row        </span>┃<span style=\"font-weight: bold\"> row       </span>┃<span style=\"font-weight: bold\"> words      </span>┃  │\n",
       "│ ┡━━━━━━━━╇━━━━╇━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━┩  │\n",
       "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">Fecha </span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   0</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">2013-01-01</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">2013-01-0</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">2013-01-01</span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">2021-12-3</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">        19</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">        2</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">    621504</span> │  │\n",
       "│ │        │    │      │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">00:00:00  </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">1        </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">00:00:00  </span> │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">1        </span> │            │           │            │  │\n",
       "│ │        │    │      │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">00:00:00 </span> │            │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">23:45:00 </span> │            │           │            │  │\n",
       "│ └────────┴────┴──────┴────────────┴───────────┴────────────┴───────────┴────────────┴───────────┴────────────┘  │\n",
       "╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n",
       "│ \u001b[3m         Data Summary         \u001b[0m \u001b[3m      Data Types       \u001b[0m                                                          │\n",
       "│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n",
       "│ ┃\u001b[1;36m \u001b[0m\u001b[1;36mDataframe        \u001b[0m\u001b[1;36m \u001b[0m┃\u001b[1;36m \u001b[0m\u001b[1;36mValues\u001b[0m\u001b[1;36m \u001b[0m┃ ┃\u001b[1;36m \u001b[0m\u001b[1;36mColumn Type\u001b[0m\u001b[1;36m \u001b[0m┃\u001b[1;36m \u001b[0m\u001b[1;36mCount\u001b[0m\u001b[1;36m \u001b[0m┃                                                          │\n",
       "│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n",
       "│ │ Number of rows    │ 310752 │ │ float64     │ 3     │                                                          │\n",
       "│ │ Number of columns │ 4      │ │ string      │ 1     │                                                          │\n",
       "│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n",
       "│ \u001b[3m                                                    number                                                    \u001b[0m  │\n",
       "│ ┏━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━━━┓  │\n",
       "│ ┃\u001b[1m \u001b[0m\u001b[1mcolumn      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNA  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNA %                \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmean  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1msd    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp0   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp25 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp50 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp75  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp100\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mhist  \u001b[0m\u001b[1m \u001b[0m┃  │\n",
       "│ ┡━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━━━┩  │\n",
       "│ │ \u001b[38;5;141mHumedad_rel \u001b[0m │ \u001b[36m 291\u001b[0m │ \u001b[36m 0.09364380599320359\u001b[0m │ \u001b[36m 62.07\u001b[0m │ \u001b[36m 26.42\u001b[0m │ \u001b[36m  3.9\u001b[0m │ \u001b[36m  40\u001b[0m │ \u001b[36m65.1\u001b[0m │ \u001b[36m 85.9\u001b[0m │ \u001b[36m 100\u001b[0m │ \u001b[32m▂▄▄▅▅█\u001b[0m │  │\n",
       "│ │ \u001b[38;5;141mTemp_aire   \u001b[0m │ \u001b[36m 348\u001b[0m │ \u001b[36m 0.11198640716713006\u001b[0m │ \u001b[36m 16.59\u001b[0m │ \u001b[36m 6.374\u001b[0m │ \u001b[36m-17.5\u001b[0m │ \u001b[36m12.4\u001b[0m │ \u001b[36m16.2\u001b[0m │ \u001b[36m 21.4\u001b[0m │ \u001b[36m35.2\u001b[0m │ \u001b[32m  ▂█▆▁\u001b[0m │  │\n",
       "│ │ \u001b[38;5;141mRad_Solar   \u001b[0m │ \u001b[36m 213\u001b[0m │ \u001b[36m 0.06854340438677788\u001b[0m │ \u001b[36m 154.5\u001b[0m │ \u001b[36m 217.2\u001b[0m │ \u001b[36m    0\u001b[0m │ \u001b[36m   0\u001b[0m │ \u001b[36m 1.5\u001b[0m │ \u001b[36m292.1\u001b[0m │ \u001b[36m1094\u001b[0m │ \u001b[32m █▁▂▁ \u001b[0m │  │\n",
       "│ └──────────────┴──────┴──────────────────────┴────────┴────────┴───────┴──────┴──────┴───────┴──────┴────────┘  │\n",
       "│ \u001b[3m                                                    string                                                    \u001b[0m  │\n",
       "│ ┏━━━━━━━━┳━━━━┳━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┓  │\n",
       "│ ┃\u001b[1m        \u001b[0m┃\u001b[1m    \u001b[0m┃\u001b[1m      \u001b[0m┃\u001b[1m            \u001b[0m┃\u001b[1m           \u001b[0m┃\u001b[1m            \u001b[0m┃\u001b[1m           \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mchars per \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mwords per\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mtotal     \u001b[0m\u001b[1m \u001b[0m┃  │\n",
       "│ ┃\u001b[1m \u001b[0m\u001b[1mcolumn\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNA\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNA %\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mshortest  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlongest  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmin       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmax      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrow       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrow      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mwords     \u001b[0m\u001b[1m \u001b[0m┃  │\n",
       "│ ┡━━━━━━━━╇━━━━╇━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━┩  │\n",
       "│ │ \u001b[38;5;141mFecha \u001b[0m │ \u001b[36m 0\u001b[0m │ \u001b[36m   0\u001b[0m │ \u001b[38;5;141m2013-01-01\u001b[0m │ \u001b[38;5;141m2013-01-0\u001b[0m │ \u001b[38;5;141m2013-01-01\u001b[0m │ \u001b[38;5;141m2021-12-3\u001b[0m │ \u001b[36m        19\u001b[0m │ \u001b[36m        2\u001b[0m │ \u001b[36m    621504\u001b[0m │  │\n",
       "│ │        │    │      │ \u001b[38;5;141m00:00:00  \u001b[0m │ \u001b[38;5;141m1        \u001b[0m │ \u001b[38;5;141m00:00:00  \u001b[0m │ \u001b[38;5;141m1        \u001b[0m │            │           │            │  │\n",
       "│ │        │    │      │            │ \u001b[38;5;141m00:00:00 \u001b[0m │            │ \u001b[38;5;141m23:45:00 \u001b[0m │            │           │            │  │\n",
       "│ └────────┴────┴──────┴────────────┴───────────┴────────────┴───────────┴────────────┴───────────┴────────────┘  │\n",
       "╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets take a look at the data\n",
    "skim(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "773337ee-8db3-4d83-baa9-0a4ebf82a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's fill in the missing values by the simplest imputation method. There don't seem to be any significant outliers,\n",
    "#so we won't address them\n",
    "for col in ['Temp_aire', 'Rad_Solar', 'Humedad_rel']:\n",
    "    data[col] = data[col].fillna(data[col].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beea4ddc-486c-4add-b654-26e880e2f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data[['Rad_Solar', 'Humedad_rel']]\n",
    "Y=data['Temp_aire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e243d332-881e-4349-95a8-1d6864610189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of obs:  310752\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of obs: \",X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5dd1905d-39a7-4d06-8467-7217e62ff895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's stay with just 10% of the data for manegability\n",
    "combined = data[['Rad_Solar', 'Humedad_rel', 'Temp_aire']]\n",
    "\n",
    "# Sample 10% of the combined DataFrame\n",
    "sampled_combined = combined.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Separate the sampled data back into X and Y\n",
    "X_sampled = sampled_combined[['Rad_Solar', 'Humedad_rel']]\n",
    "Y_sampled = sampled_combined[['Temp_aire']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf8d20-99ed-42ad-b2af-ae2a7f3f4bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "DRF = drf(min_node_size = 15, num_trees = 2000, splitting_rule = \"FourierMMD\") #those are the default values\n",
    "DRF.fit(X_sampled, Y_sampled)\n",
    "DRF.info() #prints variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c577db3b-b8a2-4398-aed7-77ae780d359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate test data\n",
    "sampled_combined = combined.sample(frac=0.01, random_state=42)\n",
    "\n",
    "# Separate the sampled data back into X and Y\n",
    "X_test= sampled_combined[['Rad_Solar', 'Humedad_rel']]\n",
    "Y_test= sampled_combined[['Temp_aire']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a38636-bf59-477a-ad7d-5630bf72e1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimated conditional distribution represented via weights\n",
    "out = DRF.predict(newdata = X_test)\n",
    "#print(out.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74820e4d-a3a7-484f-a42c-8cb8af455d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# many distributional functionals are implemented and do not need to be manually computed from the weights\n",
    "out = DRF.predict(newdata = X_test, functional = \"mean\")\n",
    "print(out.mean[0:10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
